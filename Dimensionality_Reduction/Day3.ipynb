{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6"
    },
    "pycharm": {
      "stem_cell": {
        "cell_type": "raw",
        "metadata": {
          "collapsed": false
        },
        "source": []
      }
    },
    "colab": {
      "name": "AdvanceML - ExpAcademy - Part2.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/matteoalberti/Lectures_introUML_Experis2020/blob/main/Dimensionality_Reduction/Day3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CBLBwiI48QNM"
      },
      "source": [
        "# Introduction to Autoencoder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3rzVKCR48QNM"
      },
      "source": [
        "   Email: m.alberti@deeplearningitalia.com\n",
        "\n",
        "   Linkedin:\n",
        "   [linkedin_matteo_alberti](www.linkedin.com/in/matteo-alberti-695570110)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DcyruL_M8QNM"
      },
      "source": [
        "# Summary\n",
        "\n",
        "- <font color=BE3315>**Brief Recap** </font> \n",
        "- <font color=C24024>**AutoEncoder [AE] Overview** </font> \n",
        "- <font color=CA4A2F>**Information Theory Notes** </font>\n",
        "\n",
        "- <font color=E15234>**[AE] AutoEncoder Variants** </font> \n",
        "\n",
        "- <font color=E35F2A>**Exercises** </font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KfERpSWN8QNM"
      },
      "source": [
        "# Brief Recap"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9jwPHklBSHxM"
      },
      "source": [
        "### Principal Component Analysis\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pyAL2InbV3VW"
      },
      "source": [
        "**Step-by-Step**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PEA2BZXpV1hF"
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j4qkFaJacw3w",
        "outputId": "e4f772d6-515c-4507-b56f-9a923f6b8693"
      },
      "source": [
        "X = np.random.randint(4,size=(4,4));X"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[2, 2, 1, 2],\n",
              "       [1, 3, 0, 3],\n",
              "       [3, 1, 1, 0],\n",
              "       [3, 2, 3, 1]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qg1c6d2RX2Op",
        "outputId": "830f7e1b-27a1-454c-ecab-4a444575f6a0"
      },
      "source": [
        "#Standardize\n",
        "X_scal = StandardScaler().fit_transform(X); X_scal"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-0.30151134,  0.        , -0.22941573,  0.4472136 ],\n",
              "       [-1.50755672,  1.41421356, -1.14707867,  1.34164079],\n",
              "       [ 0.90453403, -1.41421356, -0.22941573, -1.34164079],\n",
              "       [ 0.90453403,  0.        ,  1.60591014, -0.4472136 ]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fs5pBxUrYsjU"
      },
      "source": [
        "*Covariance measures how two features vary with each other*\n",
        "\n",
        " For two feature vectors $x_{j}$  and $x_{k}$ the covariance between them $\\sigma_{jk}$ can be calculated using the following equation:\n",
        "\n",
        "<img src=\"https://www.oreilly.com/library/view/r-statistics-cookbook/9781789802566/assets/76a7f6ab-d154-4882-87cd-bfc020566108.png\" width=\"300\">\n",
        "\n",
        "\n",
        "<img src=\"https://freedomtowin.github.io/non_gauss_images/output_0.png\" width=\"300\">\n",
        "\n",
        "\n",
        "While our features were standardize :\n",
        "\n",
        "$$ \\sum = \\frac{1}{n-1} X^T X $$\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y-MWeHaLYBbB",
        "outputId": "40b09d8f-180b-460a-d8be-a7d637b8bbbb"
      },
      "source": [
        "#Computing the covariance matrix\n",
        "\n",
        "cov = np.cov(X_scal); cov"
      ],
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0.11392956,  0.43994057, -0.2878807 , -0.26598942],\n",
              "       [ 0.43994057,  2.46198514, -1.615942  , -1.28598371],\n",
              "       [-0.2878807 , -1.615942  ,  1.19614926,  0.70767344],\n",
              "       [-0.26598942, -1.28598371,  0.70767344,  0.84429969]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9BHbimAleBdE",
        "outputId": "4a63c978-87fc-4462-d595-74d51098ca60"
      },
      "source": [
        "cov.shape"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(4, 4)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FIj7isOSb14K"
      },
      "source": [
        "### Eigendecomposition\n",
        "\n",
        "- The **eigenvectors** represent the principal components (**the directions of maximum variance**) of the covariance matrix\n",
        "\n",
        "- The **eigenvalues** are their corresponding **magnitude**\n",
        "\n",
        "\n",
        "*Eingenvectors and Eingenvalues are related :*\n",
        "\n",
        "$$Σv=λv$$\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rrv-qZsrYviC"
      },
      "source": [
        "# Eigendecomposition\n",
        "\n",
        "eigvals, eigvects = np.linalg.eig(cov)"
      ],
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_ORXMVf-ch1I",
        "outputId": "3657fec4-92e3-41fa-826c-c8dd523c1c98"
      },
      "source": [
        "print(eigvals.shape, eigvects.shape)"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(4,) (4, 4)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KdRXt_S4eGXQ",
        "outputId": "e266567e-1716-4431-f97c-31db3fe36d09"
      },
      "source": [
        "eigvals"
      ],
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 4.28435889e+00,  2.94681396e-01,  3.73233718e-02, -5.81049032e-17])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iwdrRkJVeIKS",
        "outputId": "ead88c0a-051f-4701-e0a4-d626f992ef10"
      },
      "source": [
        "eigvects"
      ],
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0.13967687,  0.10937857, -0.84765954,  0.5       ],\n",
              "       [ 0.75703446,  0.00671428,  0.42053983,  0.5       ],\n",
              "       [-0.50004513,  0.64240075,  0.29542537,  0.5       ],\n",
              "       [-0.3966662 , -0.75849359,  0.13169434,  0.5       ]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 81
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2_YcnzrpeY_i"
      },
      "source": [
        "### Select the number of components\n",
        "\n",
        "Sort EingenVects based on Magnitude of our EigenVals"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "63NQV6v4eZIi"
      },
      "source": [
        "idx = np.argsort(eigvals, axis=0)[::-1]\n",
        "sorted_eig_vectors = eigvects[:, idx]"
      ],
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LV3NpJ-YewjV"
      },
      "source": [
        "**How can we choose the number of components?**\n",
        "\n",
        "*plotting the cumulative sum of the eigenvalues*\n",
        "\n",
        "$$ \\frac{λ_j}{\\sum_{j=1}^d λ_j} $$\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "id": "HfQL7Ci7e0hM",
        "outputId": "933c58fd-697b-4b9e-a7bc-dd79dca83beb"
      },
      "source": [
        "cumsum = np.cumsum(eigvals[idx]) / np.sum(eigvals[idx])\n",
        "xint = range(1, len(cumsum) + 1)\n",
        "plt.plot(xint, cumsum)\n",
        "plt.xlabel(\"Number of components\")\n",
        "plt.ylabel(\"Cumulative explained variance\")\n",
        "plt.xticks(xint)\n",
        "plt.xlim(1, 4, 1)\n",
        "plt.show()"
      ],
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FTcCRGHkffAT"
      },
      "source": [
        "### Transform data into the subspace\n",
        "\n",
        "$$Z=XW$$\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vBqT5MLKfx6p",
        "outputId": "935512b3-1366-482b-9273-d86c499a1a30"
      },
      "source": [
        "Z = np.dot(X_scal, sorted_eig_vectors[:, :2]); Z"
      ],
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-0.10479046, -0.51956436],\n",
              "       [ 0.90144513, -1.9099091 ],\n",
              "       [-0.29736414,  0.95969031],\n",
              "       [-0.49929054,  1.46978315]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 84
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zZUzZ3Wv8QNM"
      },
      "source": [
        "## Autoencoder Overview\n",
        "\n",
        "*Autoencoders are a specific type of feedforward neural networks where the input is the same as the output.*\n",
        "\n",
        "![](https://miro.medium.com/max/700/1*MMRDQ4g3QvQNc7iJsKM9pg@2x.png)\n",
        "\n",
        "More technically, autoencoders are unsupervised neural networks that aim to copy their inputs to their outputs. They work by compressing the input into a **latent-space representation**, and then reconstructing the output from this representation. \n",
        "\n",
        "\n",
        "![](https://hackernoon.com/hn-images/1*8ixTe1VHLsmKB3AquWdxpQ.png)\n",
        "\n",
        "- The **latent vector** is the essence of the autoencoder;\n",
        "\n",
        "- The encoder and the decoder could be composed by any kind of layers. We will se a CNN-autoencoder;\n",
        "\n",
        "\n",
        "\n",
        "**What we need?**\n",
        "\n",
        "- An encoding method\n",
        "- A Decoding method\n",
        "- Loss function *to compare the output with the target*\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_8kWzQUIiijj"
      },
      "source": [
        "## Introduce the main layers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uxDEAwgAic_j"
      },
      "source": [
        "\n",
        "\n",
        "**Encoder:** This is the part of the network that compresses the input into a latent-space representation. It can be represented by an encoding function \n",
        "\n",
        "### $$h=f(x).$$\n",
        "\n",
        "**Decoder**: This part aims to reconstruct the input from the latent space representation. It can be represented by a decoding function \n",
        "\n",
        "### $$r=g(h).$$\n",
        "\n",
        "\n",
        "The autoencoder as a whole can thus be described by the function \n",
        "\n",
        "### $$g(f(x)) = r.$$ \n",
        "\n",
        "where $r$ is called **reconstruction**. By training the autoencoder to copy the input to the output ($r$ as close as the original input $x$), the latent representation $h$ will take on useful properties. For instance, if the dimension of $h$ is smaller than $x$ we can construct a representation of the input which is compressed. This forces the autoencoder to learn the most salient features of the training data (a sort of PCA).\n",
        "\n",
        "References:\n",
        "- [Gradient-based learning applied to document recognition](http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf). Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner. Proceedings of the IEEE, 86(11):2278-2324, November 1998.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-RGxyLb_jJzr"
      },
      "source": [
        "## Let's code together a simple autoencoder!\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nvZ8uI6QjRka"
      },
      "source": [
        "import tensorflow as tf"
      ],
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s5GSVEKsjJQc",
        "outputId": "78db7beb-8d05-445f-b7aa-6ed477a4fb1e"
      },
      "source": [
        "input_size = 784\n",
        "hidden_size = 128\n",
        "code_size = 32\n",
        "\n",
        "#Encoder\n",
        "input_img = tf.keras.Input(shape=(input_size,))\n",
        "hidden_1 = tf.keras.layers.Dense(hidden_size, activation='relu')(input_img)\n",
        "\n",
        "#Encoding Dim\n",
        "code = tf.keras.layers.Dense(code_size, activation='relu')(hidden_1)\n",
        "\n",
        "#Decoder\n",
        "hidden_2 = tf.keras.layers.Dense(hidden_size, activation='relu')(code)\n",
        "output_img = tf.keras.layers.Dense(input_size, activation='sigmoid')(hidden_2)\n",
        "\n",
        "autoencoder = tf.keras.Model(input_img, output_img)\n",
        "autoencoder.compile(optimizer='adam', loss='binary_crossentropy')\n",
        "\n",
        "autoencoder.summary()"
      ],
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"functional_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         [(None, 784)]             0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 128)               100480    \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 32)                4128      \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 128)               4224      \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 784)               101136    \n",
            "=================================================================\n",
            "Total params: 209,968\n",
            "Trainable params: 209,968\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NNtgoDnh8QNM"
      },
      "source": [
        "## <h1><center>  Information Theory Background \n",
        "\n",
        "\n",
        "### Self-Information\n",
        "\n",
        "The idea behind self-information is the following\n",
        "\n",
        "* **if an event always occurs, we associate it with a smaller amount of information. It will not suprise us!**\n",
        "* **On the other side, a rare event is associated with a huge amount of information. It will suprise us!**\n",
        "\n",
        "I am not surprise to see the sunrise every morning (likely event). Instead,  I would be really suprised if tomorriw the Sun will not rise (unlikely event). This amount of surprise or self-information of the event $x$ is quantified by\n",
        "\n",
        "$$I(x) = - \\log p(x),$$\n",
        "\n",
        "where $p(x)$ is the probability of the event $x$. If $p(x)=1$, then self-info is zero. A rare event instead has a huge surpise factor.\n",
        "\n",
        "### Shannon Entropy \n",
        "\n",
        "In terms of self-info, **Shannon Entropy** is the is the average self-information (**expected value**) over all possible values of X.\n",
        "The entropy for a probability $p(x)$ distribution is\n",
        "\n",
        "$$ S = - \\sum_i p(x_i) \\log p(x_i),$$\n",
        "\n",
        "where we assume we know the probability $p$ for each outcome $i$. If we use $log_2$  for our calculation we can **interpret entropy as *the minimum number of bits it would take us to encode our information*.**\n",
        "\n",
        "For continous variables, we can use the integral form\n",
        "\n",
        "$$ S = - \\int  p(x) \\log p(x) \\, dx,$$\n",
        "\n",
        "where now $p(x)$ is take the role of a **probability density function** (PDF). Take in mind that a broad probability density has higher entropy than an narrowed one (think about Gaussian distribution vs delta Dirac, which has $S=0$).\n",
        "\n",
        "In both discrete and continous formulation, we are computing the expectation (i.e. average) of the negative log-probability (i.e. self-info) which is the theoretical minimum encoding size of the information from the event $x$. The same formula is usually written as\n",
        "\n",
        "$$S = \\mathbb E _{\\, x \\sim p} \\left[ -\\log p(x) \\right],$$\n",
        "\n",
        "where $x \\sim p$ means that we calculate the expectation with the probability distribution $p$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZJRv-SzQ8QNM"
      },
      "source": [
        "Let's say we have to pass a message about what drink Matteo would take during the week. In general, Matteo loves *Puer Tea* , Green Tea, black Tea and Oolong Tea.\n",
        "\n",
        "![Puer Tea](https://c.ndtvimg.com/sc8uey7at2h_puerh-tea_625x300.jpg) \n",
        "\n",
        "-----------------------\n",
        "*Small Recap :*\n",
        "\n",
        "- Black Tea : Twinings (like) - you can eat some biscuits\n",
        "- Green Tea and Oolong : really good if you wanna relax (evening)\n",
        "- Puer : only in special occasions (take your time)\n",
        "\n",
        "-----------------------\n",
        "$$$$\n",
        "\n",
        "\n",
        "Usually Matteo has not preferences about the tea type (*not real*) the probability distribution of his choice is: \n",
        "\n",
        "$$P(\\text Puer Tea ) =  P(\\text Green Tea ) = P(\\text black Tea ) = P(\\text Oolong Tea ) = 0.25,$$\n",
        "\n",
        "while the corresponding entropy\n",
        "\n",
        "$$S = - \\frac{1}{4} \\log \\frac{1}{4} - \\frac{1}{4} \\log \\frac{1}{4} - \\frac{1}{4} \\log \\frac{1}{4} - \\frac{1}{4} \\log \\frac{1}{4} = 2$$\n",
        "\n",
        "In the morning, he usually is late and wanna have breakfast : \n",
        "\n",
        "$$P(\\text Puer Tea ) = 0.125,\\;  P(\\text Green Tea ) =0.125,\\;  P(\\text black Tea ) = 0.5,\\; P(\\text  Oolong Tea ) = 0.25,$$\n",
        "\n",
        "while the corresponding entropy\n",
        "\n",
        "$$S = - \\frac{1}{8} \\log \\frac{1}{8} - \\frac{1}{8} \\log \\frac{1}{8} - \\frac{1}{2} \\log \\frac{1}{2} - \\frac{1}{4} \\log \\frac{1}{4} = 1.75$$\n",
        "\n",
        "\n",
        "On Sunday late afternoon, Matteo often looks for some relax on his sofa\n",
        "\n",
        "$$P(\\text Puer Tea ) = 0.012,\\;  P(\\text Green Tea ) =0.52,\\;  P(\\text black Tea ) = 0.018,\\; P(\\text  Oolong Tea ) = 0.45,$$\n",
        "\n",
        "and the corresponding entropy\n",
        "\n",
        "$$S = - 0.012 \\log 0.012 - 0.52 \\log 0.52 - 0.018 \\log 0.018 - 0.45 \\log 0.45 = 1.365$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SeX6vEH28QNN"
      },
      "source": [
        "**If entropy is high (encoding size $log_2 p(x)$ is big on average)**, **it means we have many message types with small and almost equal probabilities**. Hence, every time a new message arrives, you would expect a different type than previous messages. You may see it as a disorder or uncertainty or unpredictability.\n",
        "\n",
        "On the contrary, **when a message has much smaller probability than other message, it appears as a surprise because on average you would expect other more frequently sent message types**. Also, a rare message type has more information than more frequent message types because it eliminates a lot of other probabilities and tells us more specific information.\n",
        "\n",
        "If the entropy is high, the average encoding size is significant which means each message tends to have more (specific) information. Again, this is why high entropy is associated with disorder, uncertainty, surprise, unpredictability, amount of information. The more random a message is, the more information will be gained from decoding the message.\n",
        "\n",
        "**Low entropy means that most of the times we are receiving the more predictable information which means less disorder**, less uncertainty, less surprise, more predictability and less (specific) information. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wUPRSl8j8QNN"
      },
      "source": [
        "### Cross Entropy\n",
        "\n",
        "**Suppose to have two distributions**, the true one $p(x)$ and the estimated $q(x)$. In the language of neural networks, $p(x)$ would be the grond truth (labels in one hot-encoding) and $q(x)$ the outcome of the net, i.e. the one that your machine learning algorithm is trying to match. **Cross entropy is a mathematical tool for comparing two probability distributions** $p(x)$ and $q(x)$ and it is expressed by the formula \n",
        "\n",
        "$$ H (p,q) = - \\int p(x) \\log q(x)\\,dx.$$\n",
        "\n",
        "If $\\log$ is in base $2$, then cross entropy measures the **number of bits you will need encoding symbols** from $p$ using the wrong distribution $q$. Subtracting to cross entropy the entropy of $p$, you are counting the cost in terms of bits of using the wrong distribution $q$ (this somehow will be KL-divergence). "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XtWvpW_L8QNN",
        "outputId": "ba8598ef-434f-4c63-9d4e-b19137261146"
      },
      "source": [
        "# Import scipy and numpy module\n",
        "import scipy.stats\n",
        "import numpy as np\n",
        "\n",
        "# Create an array\n",
        "a=np.array([1,1,2,3,1,3,4,2,5,6,3,2,4,3])\n",
        "\n",
        "\n",
        "# Compute probability distribution\n",
        "a_pdf=scipy.stats.norm.pdf(a)\n",
        "\n",
        "# Calculate the entropy of a distribution for given probability values.\n",
        "entropy = scipy.stats.entropy(a_pdf) # get entropy from probability values\n",
        "print(\"Entropy: \",entropy)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Entropy:  1.6688066853941022\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wIcbz1cG8QNO"
      },
      "source": [
        "### Kullback-Leibler Divergence\n",
        "\n",
        "Kullback–Leibler divergence is the relative entropy of two probability distributions. It measures the distance(similarity or dissimilarity) of one distribution from another reference probability distribution. 0 Value of Kullback–Leibler divergence indicates that both distributions are identical. It can be expressed as : \n",
        "\n",
        "$$D_{KL}(h || g) = - \\sum_i h(x_i) (\\log h(x) - \\log g(x)) = - \\sum_i h(x_i) \\log \\frac{h(x)}{g(x)}$$ \n",
        "\n",
        "- **not a measure : it's asymmetric**\n",
        "\n",
        "\n",
        "In the variational autoencoder loss function, the KL-divergence is used to force the distribution of latent variables  q(z|x)  to be a normal distribution  n(z)  so that we can sample latent variables from the normal distribution. As such, the KL-divergence is included in the loss function to improve the similarity between the distribution of latent variables and the normal distribution"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kdgxG4yI8QNO",
        "outputId": "302efd62-ee3a-47f2-899c-127a1b3a0665"
      },
      "source": [
        "# Import scipy.stats and numpy module\n",
        "import scipy.stats\n",
        "import numpy as np\n",
        "\n",
        "# Create numpy arrays\n",
        "a=np.array([1,1,2,3,1,3,4,2,5,6,3,2,4,3])\n",
        "b=np.array([1,1,3,4,2,4,5,2,5,6,3,2,4,3])\n",
        "\n",
        "# Compute probability distribution\n",
        "a_pdf=scipy.stats.norm.pdf(a)\n",
        "b_pdf=scipy.stats.norm.pdf(b)\n",
        "\n",
        "# compute relative entropy or KL Divergence\n",
        "kl_div=scipy.stats.entropy(a_pdf,b_pdf)\n",
        "\n",
        "print(\"KL Divergence: \",kl_div)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "KL Divergence:  0.26732496641464365\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E6X3topuu8nY"
      },
      "source": [
        "## AutoEncoders vs PCA\n",
        "\n",
        "*Training an autoencoder with one dense encoder layer and one dense decoder layer and linear activation is essentially equivalent to performing PCA*\n",
        "\n",
        "- An autoencoder could let you make use of pre trained layers from another model, to apply transfer learning to prime the encoder/decoder.\n",
        "\n",
        "![](https://miro.medium.com/max/700/1*P7-YLUAwXH6OI8fxh17gpA.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HzZWL1-NnTS0"
      },
      "source": [
        "## AutoEncoder variants"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "udN-fhYqnnkO"
      },
      "source": [
        "### Sparse AutoEncoder\n",
        "\n",
        "*We can regularize the autoencoder by using a sparsity constraint such that only a fraction of the nodes would have nonzero values, called active nodes.*\n",
        "\n",
        "\n",
        "![](https://miro.medium.com/max/399/1*kj2xw4MJLNOKZ_-qLTbDPQ.png)\n",
        "\n",
        "$$$$\n",
        "\n",
        "-------------------------------\n",
        "**L2 & L1 regularization**\n",
        "\n",
        "*These update the general cost function by adding another term known as the regularization term.*\n",
        "\n",
        "*Due to the addition of this regularization term, the values of weight matrices decrease because it assumes that a neural network with smaller weight matrices leads to simpler models.*\n",
        "\n",
        "**Therefore, it will also reduce overfitting to quite an extent.**\n",
        "\n",
        "*An Example :*\n",
        "\n",
        "![](https://cdn.analyticsvidhya.com/wp-content/uploads/2018/04/Screen-Shot-2018-04-04-at-1.59.54-AM.png)\n",
        "\n",
        "---------------------------------------------\n",
        "\n",
        "\n",
        "*Similar to Dropout*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0pheHefAnTb9",
        "outputId": "f0529f03-6123-473f-e512-49d68526d7f3"
      },
      "source": [
        "#Encoder\n",
        "input_img = tf.keras.Input(shape=(input_size,))\n",
        "hidden_1 = tf.keras.layers.Dense(hidden_size, activation='relu', activity_regularizer=tf.keras.regularizers.L1(10e-6))(input_img)\n",
        "\n",
        "#Encoding Dim\n",
        "code = tf.keras.layers.Dense(code_size, activation='relu', activity_regularizer=tf.keras.regularizers.L1(10e-6))(hidden_1)\n",
        "\n",
        "#Decoder\n",
        "hidden_2 = tf.keras.layers.Dense(hidden_size, activation='relu', activity_regularizer=tf.keras.regularizers.L1(10e-6))(code)\n",
        "output_img = tf.keras.layers.Dense(input_size, activation='sigmoid', activity_regularizer=tf.keras.regularizers.L1(10e-6))(hidden_2)\n",
        "\n",
        "sparse_autoencoder = tf.keras.Model(input_img, output_img)\n",
        "sparse_autoencoder.compile(optimizer='adam', loss='binary_crossentropy')\n",
        "\n",
        "sparse_autoencoder.summary()"
      ],
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"functional_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_4 (InputLayer)         [(None, 784)]             0         \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 128)               100480    \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 32)                4128      \n",
            "_________________________________________________________________\n",
            "dense_6 (Dense)              (None, 128)               4224      \n",
            "_________________________________________________________________\n",
            "dense_7 (Dense)              (None, 784)               101136    \n",
            "=================================================================\n",
            "Total params: 209,968\n",
            "Trainable params: 209,968\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c151-YQkrcmM"
      },
      "source": [
        "### Stacked Autoencoders\n",
        "\n",
        "$$$$\n",
        "\n",
        "-------------------------------------------\n",
        "\n",
        "<img src=\"https://miro.medium.com/max/700/1*7H9VQlN94-wv7Ianqt6GZg.png\" width=500\\>\n",
        "\n",
        "\n",
        "*Nothing to say. Just a deeper version : ANN --> MLP*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BZC5LRJmsKNT"
      },
      "source": [
        "### Variational AutoEncoders\n",
        "\n",
        "\n",
        "$$$$\n",
        "\n",
        "-------------------------------------------\n",
        "\n",
        "![](https://www.analyticsindiamag.com/wp-content/uploads/2019/08/vae-768x474.png)\n",
        "\n",
        "-------------------------------------------\n",
        "\n",
        "$$$$\n",
        "\n",
        "**Basic Idea**\n",
        "\n",
        "*Instead of mapping an input to fixed vector, input is mapped to a distribution*\n",
        "\n",
        "**bottleneck vector** is replaced with two different vectors one representing the **mean** of the distribution and the other representing the **standard deviation** of the distribution.\n",
        "\n",
        "*We have to define a new Loss Function :*\n",
        "\n",
        "$$ l​oss​​(θ,ϕ)=−E​z∼q​θ​​(z∣x​i​​)​​[logp​ϕ​​(x​i​​∣z)]+KL(q​θ​​(z∣x​i​​)∣∣p(z)) $$\n",
        "\n",
        "*Where :*\n",
        "\n",
        "- $ q​θ​​(z∣x​i​​) : $ Reconstruction Loss\n",
        "- $ [logp​ϕ​​(x​i​​∣z)] :$ Regularizer\n",
        "- $ KL(q​θ​​(z∣x​i​​)∣∣p(z) : $ KL divergence (encoder distribution vs $p(z)$\n",
        "  - divergence measures how much information is lost when using q to represent p"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1UkOid2pvarJ"
      },
      "source": [
        "# Domain Application"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TsKA_bfK8QNO"
      },
      "source": [
        "##  [Computer Vision - Image Recontruction]\n",
        "\n",
        "      Choose and Load a dataset (mnist, cifar10, cifar100, fashion_mnist)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dokPyJryJeP4",
        "outputId": "1ec34488-bd2d-44f4-ceb0-a022c61dfa71"
      },
      "source": [
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.fashion_mnist.load_data()\n",
        "\n",
        "x_train = x_train.astype('float32') / 255.\n",
        "x_test = x_test.astype('float32') / 255.\n",
        "\n",
        "print (x_train.shape)\n",
        "print (x_test.shape)"
      ],
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(60000, 28, 28)\n",
            "(10000, 28, 28)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v7PBYh9dvqir"
      },
      "source": [
        "### Apply Simple Autoencoder and reconstruct the pictures"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "74OJsimVwFCu"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sr-5BKr3wEid"
      },
      "source": [
        "### Apply Convolutional Autoencoder and reconstruct the pictures"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZTJ5EhrqwGyq"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZZoKTxiYxG2P"
      },
      "source": [
        "##  [Tabular Data - Feature Extractor]\n",
        "\n",
        "      Open your previous exercise Notebook (Day2_exercise.ipynb) and test a basic autoencoder in the following way :\n",
        "\n",
        "      - Reduce your data with PCA\n",
        "      - Reduce your data with AE\n",
        "\n",
        "      Apply Custering / Classification/ Regression from this two projected version and evaluate performances depending on your dataset type\n",
        "\n",
        "\n",
        "      **** Tips : I need some help!   \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oMLh_sDVxG_w"
      },
      "source": [
        "# load your data X and make preprocessing (like scaling)\n",
        "\n",
        "# apply PCA / AE like :\n",
        "\n",
        "pca = PCA(0.9).fit(X)\n",
        "X_projected = pca.transform(X)\n",
        "\n",
        "# Now \"X_projected\" is my new set of features! Check multicollinerity\n",
        "\n",
        "# for clustering\n",
        "kmeans = KMeans(n_clusters=10,init='k-means++',max_iter=300, n_init=12, random_state=0).fit(X_projected) # just an example\n",
        "\n",
        "# for classification, Please develop a Neural Network with Dense!\n",
        "log_regressor=LogisticRegression(multi_class='multinomial') \n",
        "log_regressor.fit(X_projected, Y_train)\n",
        "\n",
        "# for regression, Please develop a Neural Network with Dense!\n",
        "lin_regressor=LinearRegression()\n",
        "lin_regressor.fit(X_projected, Y_train)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}